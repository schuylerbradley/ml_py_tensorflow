{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    VisualizeConvnetHeatMapVGG16\n",
    "    Only downloads VGG16 the first time!  After that it seems to find it\n",
    "'''\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = VGG16(weights = 'imagenet')\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# found photo on https://unsplash.com/search/photos/elephants\n",
    "# It starts out way too detailed:  58.6 kB (58,569 bytes)\n",
    "#img_path = '/home/sbradley/tensorflow/mlenv/chollet_book/images/photo-elephant-creative-commons.jpeg'\n",
    "img_path = '/home/sbradley/tensorflow/mlenv/chollet_book/images/african-elephant-antoine-pluss.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size:  (224, 224)\n",
      "x =image.img_to_array size:  150528\n",
      "x =preprocess_input size:  150528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Now 4-D and some values are negative...\\n[[[[  66.061       22.221      -17.68     ]\\n   [  66.061       22.221      -17.68     ]\\n   ...\\n   [ 108.061       79.221       55.32     ]]\\n\\n  [[  55.060997    10.221001   -31.68     ]\\n  ... '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('image size: ', img.size) # image size:  (224, 224) so above really works!!\n",
    "x = image.img_to_array(img) # 224 * 224 = 50176, but this is RBG, so 3 * 50176 = 150528!\n",
    "#print('image values: ', x[[0,0]]) \n",
    "''' image values is a 3-D list of lists of lists height, width, depth = color  \n",
    "[[[106. 139. 170.]\n",
    "  [106. 139. 170.]\n",
    "  [106. 141. 173.]\n",
    "  ...\n",
    "  [177. 196. 213.]\n",
    "  [178. 195. 211.]\n",
    "  [179. 196. 212.]]\n",
    "\n",
    " [[106. 139. 170.]...'''\n",
    "print('x =image.img_to_array size: ', x.size)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x) # normalize colors\n",
    "print('x =preprocess_input size: ', x.size)\n",
    "#print('preprocess_input image values: ', x[[0,0]]) \n",
    "''' Now 4-D and some values are negative...\n",
    "[[[[  66.061       22.221      -17.68     ]\n",
    "   [  66.061       22.221      -17.68     ]\n",
    "   ...\n",
    "   [ 108.061       79.221       55.32     ]]\n",
    "\n",
    "  [[  55.060997    10.221001   -31.68     ]\n",
    "  ... '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [('n02504458', 'African_elephant', 0.9300823), ('n01871265', 'tusker', 0.051791735), ('n02504013', 'Indian_elephant', 0.018125663)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Predicted: ', decode_predictions(preds, top=3)[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_conv_layer.output:  (?, 14, 14, 512)\n",
      "grads values where 512 is the number of channels:  Tensor(\"gradients_1/block5_pool/MaxPool_grad/MaxPoolGrad:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "value_index:  <tensorflow.python.framework.ops.Graph object at 0x7fe22579d128>\n"
     ]
    }
   ],
   "source": [
    "african_elephant_ouput = model.output[:, 386]\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "print('last_conv_layer.output: ', last_conv_layer.output.shape) \n",
    "grads = K.gradients(african_elephant_ouput, last_conv_layer.output)[0]\n",
    "\n",
    "#print('grads size: ', grads.size)\n",
    "print('grads values where 512 is the number of channels: ', grads) \n",
    "# So grads shape matches last_conv_layer.output\n",
    "print('value_index: ', grads.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads = K.mean(grads, axis=(0,1,2))\n",
    "iterate = K.function([model.input],\n",
    "                    [pooled_grads, last_conv_layer.output[0]])\n",
    "pooled_grads_value, conv_layer_ouput_value = iterate([x])\n",
    "\n",
    "'''\n",
    "for np.mean(..axis=-1) below will refer to the final axis and average all values in this axis.\n",
    "Adding to the excellent answer by Daniel MÃ¶ller, \n",
    "if your data has a shape (19,19,5,80) then keras.max(a, axis=-1) \n",
    "would return a matrix of shape (19,19,5) where each value of the output matrix \n",
    "would be the maximum of the 80 elements (the maximum of the values specified within the last index\n",
    "'''\n",
    "for i in range(512):\n",
    "    conv_layer_ouput_value[:,:,i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(conv_layer_ouput_value, axis = -1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe2142d2be0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe21c70d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap =  np.maximum(heatmap,0)\n",
    "heatmap /= np.max(heatmap)\n",
    "\n",
    "import matplotlib.pyplot as pyplt\n",
    "pyplt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.size:  (224, 224)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Use the OpenCV library for image read, write and display!\n",
    "Use the function cv2.imshow() to display an image in a window. The window automatically fits to the image size.\n",
    "In order to import cv2 correctly, I installed as follows in mlenv:\n",
    "pip install opencv-contrib-python\n",
    "However, the following never worked on my Ubuntu machine:  pip install opencv \n",
    "'''\n",
    "import cv2\n",
    "\n",
    "cv2.imread(img_path)\n",
    "print('img.size: ', img.size)\n",
    "heatmap = cv2.resize(heatmap, (img.size[1], img.size[0]))\n",
    "heatmap = np.uint8(255 * heatmap) # scale every color of every pixel by 255\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "#pyplt.matshow(superimposed_img)\n",
    "#plt.imshow(heatmap)\n",
    "\n",
    "superimposed_grad_img_path ='/home/sbradley/tensorflow/mlenv/chollet_book/images/heat-map-loss-grad-african-elephant-img.jpg'\n",
    "cv2.imwrite(superimposed_grad_img_path, superimposed_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
